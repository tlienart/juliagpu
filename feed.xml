<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
  <title><![CDATA[JuliaGPU]]></title>
  <description><![CDATA[High-performance GPU programming in a high-level language.
]]></description>
  <link>https://juliagpu.org/</link>
  <atom:link href="https://juliagpu.org/feed.xml" rel="self" type="application/rss+xml" />
  <item>
    <title><![CDATA[CUDA.jl 2.4 and 2.5]]></title>
    <link>https://juliagpu.org/post/2021-01-08-cuda_2.4_2.5/</link>
    <description><![CDATA[&lt;p&gt;CUDA.jl v2.4 and v2.5 are two almost-identical feature releases, respectively for Julia 1.5 and 1.6. These releases feature a greatly improved &lt;code&gt;findmin&lt;/code&gt; and &lt;code&gt;findmax&lt;/code&gt; kernels, an improved interface for kernel introspection, support for CUDA 11.2, and of course many bug fixes.&lt;/p&gt; &lt;h2 id&#61;&quot;improved<em>findmin</em>and<em>findmax</em>kernels<strong>2&quot;&gt;&lt;a href&#61;&quot;#improved<em>findmin</em>and<em>findmax</em>kernels</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Improved &lt;code&gt;findmin&lt;/code&gt; and &lt;code&gt;findmax&lt;/code&gt; kernels&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Thanks to &lt;a href&#61;&quot;https://github.com/tkf&quot;&gt;@tkf&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/Ellipse0934&quot;&gt;@Ellipse0934&lt;/a&gt;, CUDA.jl now &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/576&quot;&gt;uses a single-pass kernel for finding the minimum or maximum item in a CuArray&lt;/a&gt;. This fixes compatibility with &lt;code&gt;NaN&lt;/code&gt;-valued elements, while on average improving performance. Depending on the rank, shape and size of the array these improvements vary from a minor regression to order-of-magnitude improvements.&lt;/p&gt; &lt;h2 id&#61;&quot;new<em>kernel</em>introspection<em>interface__2&quot;&gt;&lt;a href&#61;&quot;#new</em>kernel<em>introspection</em>interface__2&quot; class&#61;&quot;header-anchor&quot;&gt;New kernel introspection interface&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;It is now possible to obtain a compiled-but-not-launched kernel by passing the &lt;code&gt;launch&#61;false&lt;/code&gt; keyword to &lt;code&gt;@cuda&lt;/code&gt;. This is useful when you want to reflect, e.g., query the amount of registers, or other kernel properties:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;julia&gt; kernel &#61; @cuda launch&#61;false identity&#40;nothing&#41; CUDA.HostKernel&#123;identity,Tuple&#123;Nothing&#125;&#125;&#40;...&#41;
julia&gt; CUDA.registers&#40;kernel&#41; 4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The old API is still available, and will even be extended in future versions of CUDA.jl for the purpose of compiling device functions &#40;not kernels&#41;:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;julia&gt; kernel &#61; cufunction&#40;identity, Tuple&#123;Nothing&#125;&#41; CUDA.HostKernel&#123;identity,Tuple&#123;Nothing&#125;&#125;&#40;...&#41;&lt;/code&gt;&lt;/pre&gt; &lt;h2 id&#61;&quot;support<em>for</em>cuda<em>112__2&quot;&gt;&lt;a href&#61;&quot;#support</em>for<em>cuda</em>112__2&quot; class&#61;&quot;header-anchor&quot;&gt;Support for CUDA 11.2&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDA.jl now supports the latest version of CUDA, version 11.2. Because CUDNN and CUTENSOR are not compatible with this release yet, CUDA.jl won&#39;t automatically switch to it unless you explicitly request so:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;julia&gt; ENV&#91;&quot;JULIA<em>CUDA</em>VERSION&quot;&#93; &#61; &quot;11.2&quot; &quot;11.2&quot;
julia&gt; using CUDA
julia&gt; CUDA.versioninfo&#40;&#41; CUDA toolkit 11.2.0, artifact installation CUDA driver 11.2.0 NVIDIA driver 460.27.4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, if you disable use of artifacts through &lt;code&gt;JULIA<em>CUDA</em>USE<em>BINARYBUILDER&#61;false&lt;/code&gt;, CUDA 11.2 can be picked up from your local system.&lt;/p&gt; &lt;h2 id&#61;&quot;future</em>developments<strong>2&quot;&gt;&lt;a href&#61;&quot;#future_developments</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Future developments&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Due to upstream compiler changes, CUDA.jl 2.4 is expected to be the last release compatible with Julia 1.5. Patch releases are still possible, but are not automatic: If you need a specific bugfix from a future CUDA.jl release, create an issue or PR to backport the change.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2021-01-08-cuda_2.4_2.5/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2021-01-08-cuda_2.4_2.5/</guid>
    <pubDate>Fri, 8 Jan 2021 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[Introducing: oneAPI.jl]]></title>
    <link>https://juliagpu.org/post/2020-11-05-oneapi_0.1/</link>
    <description><![CDATA[&lt;p&gt;We&#39;re proud to announce the first version of oneAPI.jl, a Julia package for programming accelerators with the &lt;a href&#61;&quot;https://www.oneapi.com/&quot;&gt;oneAPI programming model&lt;/a&gt;. It is currently available for select Intel GPUs, including common integrated ones, and offers a similar experience to CUDA.jl.&lt;/p&gt; &lt;p&gt;The initial version of this package, v0.1, consists of three key components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;wrappers for the oneAPI Level Zero interfaces;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;a compiler for Julia source code to SPIR-V IR;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;and an array interface for convenient data-parallel programming.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In this post, I&#39;ll briefly describe each of these. But first, some essentials.&lt;/p&gt; &lt;h2 id&#61;&quot;installation<strong>2&quot;&gt;&lt;a href&#61;&quot;#installation</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Installation&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;oneAPI.jl is currently only supported on 64-bit Linux, using a sufficiently recent kernel, and requires Julia 1.5. Furthermore, it currently only supports a limited set of Intel GPUs: Gen9 &#40;Skylake, Kaby Lake, Coffee Lake&#41;, Gen11 &#40;Ice Lake&#41;, and Gen12 &#40;Tiger Lake&#41;.&lt;/p&gt; &lt;p&gt;If your Intel CPU has an integrated GPU supported by oneAPI, you can just go ahead and install the oneAPI.jl package:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;pkg&gt; add oneAPI&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That&#39;s right, no additional drivers required&#33; oneAPI.jl ships its own copy of the &lt;a href&#61;&quot;https://github.com/intel/compute-runtime&quot;&gt;Intel Compute Runtime&lt;/a&gt;, which works out of the box on any &#40;sufficiently recent&#41; Linux kernel. The initial download, powered by Julia&#39;s artifact subsystem, might take a while to complete. After that, you can import the package and start using its functionality:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; using oneAPI
julia&gt; oneAPI.versioninfo&#40;&#41; Binary dependencies:
<ul>
<li>NEO_jll: 20.42.18209&#43;0
</li>
<li>libigc_jll: 1.0.5186&#43;0
</li>
<li>gmmlib_jll: 20.3.2&#43;0
</li>
<li>SPIRV<em>LLVM</em>Translator_jll: 9.0.0&#43;1
</li>
<li>SPIRV<em>Tools</em>jll: 2020.2.0&#43;1
</li>
</ul>
Toolchain:
<ul>
<li>Julia: 1.5.2
</li>
<li>LLVM: 9.0.1
</li>
</ul>
1 driver:
<ul>
<li>00007fee-06cb-0a10-1642-ca9f01000000 &#40;v1.0.0, API v1.0.0&#41;
</li>
</ul>
1 device:
<ul>
<li>Intel&#40;R&#41; Graphics Gen9&lt;/code&gt;&lt;/pre&gt;
</li>
</ul>
&lt;h2 id&#61;&quot;the<em>onearray</em>type<strong>2&quot;&gt;&lt;a href&#61;&quot;#the<em>onearray</em>type</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;The &lt;code&gt;oneArray&lt;/code&gt; type&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Similar to CUDA.jl&#39;s &lt;code&gt;CuArray&lt;/code&gt; type, oneAPI.jl provides an array abstraction that you can use to easily perform data parallel operations on your GPU:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; a &#61; oneArray&#40;zeros&#40;2,3&#41;&#41; 2×3 oneArray&#123;Float64,2&#125;:  0.0  0.0  0.0  0.0  0.0  0.0
julia&gt; a .&#43; 1 2×3 oneArray&#123;Float64,2&#125;:  1.0  1.0  1.0  1.0  1.0  1.0
julia&gt; sum&#40;ans; dims&#61;2&#41; 2×1 oneArray&#123;Float64,2&#125;:  3.0  3.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This functionality builds on the &lt;a href&#61;&quot;https://github.com/JuliaGPU/GPUArrays.jl/&quot;&gt;GPUArrays.jl&lt;/a&gt; package, which means that a lot of operations are supported out of the box. Some are still missing, of course, and we haven&#39;t carefully optimized for performance either.&lt;/p&gt; &lt;h2 id&#61;&quot;kernel<em>programming__2&quot;&gt;&lt;a href&#61;&quot;#kernel</em>programming__2&quot; class&#61;&quot;header-anchor&quot;&gt;Kernel programming&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;The above array operations are made possible by a compiler that transforms Julia source code into SPIR-V IR for use with oneAPI. Most of this work is part of &lt;a href&#61;&quot;https://github.com/JuliaGPU/GPUCompiler.jl&quot;&gt;GPUCompiler.jl&lt;/a&gt;. In oneAPI.jl, we use this compiler to provide a kernel programming model:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; function vadd&#40;a, b, c&#41;            i &#61; get<em>global</em>id&#40;&#41;            @inbounds c&#91;i&#93; &#61; a&#91;i&#93; &#43; b&#91;i&#93;            return        end
julia&gt; a &#61; oneArray&#40;rand&#40;10&#41;&#41;;
julia&gt; b &#61; oneArray&#40;rand&#40;10&#41;&#41;;
julia&gt; c &#61; similar&#40;a&#41;;
julia&gt; @oneapi items&#61;10 vadd&#40;a, b, c&#41;
julia&gt; @test Array&#40;a&#41; .&#43; Array&#40;b&#41; &#61;&#61; Array&#40;c&#41; Test Passed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, the &lt;code&gt;@oneapi&lt;/code&gt; macro resembles &lt;code&gt;@cuda&lt;/code&gt; from CUDA.jl. One of the differences with the CUDA stack is that we use OpenCL-style built-ins, like &lt;code&gt;get<em>global</em>id&lt;/code&gt; instead of &lt;code&gt;threadIdx&lt;/code&gt; and &lt;code&gt;barrier&lt;/code&gt; instead of &lt;code&gt;sync<em>threads&lt;/code&gt;. Other familiar functionality, e.g. to reflect on the compiler, is available as well:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; @device</em>code_spirv @oneapi vadd&#40;a, b, c&#41; ; CompilerJob of kernel vadd&#40;oneDeviceArray&#123;Float64,1,1&#125;, ;                            oneDeviceArray&#123;Float64,1,1&#125;, ;                            oneDeviceArray&#123;Float64,1,1&#125;&#41; ; for GPUCompiler.SPIRVCompilerTarget
; SPIR-V ; Version: 1.0 ; Generator: Khronos LLVM/SPIR-V Translator; 14 ; Bound: 46 ; Schema: 0                OpCapability Addresses                OpCapability Linkage                OpCapability Kernel                OpCapability Float64                OpCapability Int64                OpCapability Int8 &#37;1 &#61; OpExtInstImport &quot;OpenCL.std&quot;                OpMemoryModel Physical64 OpenCL                OpEntryPoint Kernel                ...                OpReturn                OpFunctionEnd&lt;/code&gt;&lt;/pre&gt; &lt;h2 id&#61;&quot;level<em>zero</em>wrappers<strong>2&quot;&gt;&lt;a href&#61;&quot;#level<em>zero</em>wrappers</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Level Zero wrappers&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;To interface with the oneAPI driver, we use the &lt;a href&#61;&quot;https://github.com/oneapi-src/level-zero&quot;&gt;Level Zero API&lt;/a&gt;. Wrappers for this API is available under the &lt;code&gt;oneL0&lt;/code&gt; submodule of oneAPI.jl:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; using oneAPI.oneL0
julia&gt; drv &#61; first&#40;drivers&#40;&#41;&#41; ZeDriver&#40;00000000-0000-0000-1642-ca9f01000000, version 1.0.0&#41;
julia&gt; dev &#61; first&#40;devices&#40;drv&#41;&#41; ZeDevice&#40;GPU, vendor 0x8086, device 0x1912&#41;: Intel&#40;R&#41; Graphics Gen9&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a low-level interface, and importing this submodule should not be required for the vast majority of users. It is only useful when you want to perform very specific operations, like submitting an certain operations to the command queue, working with events, etc. In that case, you should refer to the &lt;a href&#61;&quot;https://spec.oneapi.com/level-zero/latest/index.html&quot;&gt;upstream specification&lt;/a&gt;; The wrappers in the &lt;code&gt;oneL0&lt;/code&gt; module closely mimic the C APIs.&lt;/p&gt; &lt;h2 id&#61;&quot;status<strong>2&quot;&gt;&lt;a href&#61;&quot;#status</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Status&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Version 0.1 of oneAPI.jl forms a solid base for future oneAPI developments in Julia. Thanks to the continued effort of generalizing the Julia GPU support in packages like GPUArrays.jl and GPUCompiler.jl, this initial version is already much more usable than early versions of CUDA.jl or AMDGPU.jl ever were.&lt;/p&gt; &lt;p&gt;That said, there are crucial parts missing. For one, oneAPI.jl does not integrate with any of the vendor libraries like oneMKL or oneDNN. That means several important operations, e.g. matrix-matrix multiplication, will be slow. Hardware support is also limited, and the package currently only works on Linux.&lt;/p&gt; &lt;p&gt;If you want to contribute to oneAPI.jl, or run into problems, check out the GitHub repository at &lt;a href&#61;&quot;https://github.com/JuliaGPU/oneAPI.jl&quot;&gt;JuliaGPU/oneAPI.jl&lt;/a&gt;. For questions, please use the &lt;a href&#61;&quot;https://discourse.julialang.org/c/domain/gpu&quot;&gt;Julia Discourse forum&lt;/a&gt; under the GPU domain and/or in the #gpu channel of the &lt;a href&#61;&quot;https://julialang.org/community/&quot;&gt;Julia Slack&lt;/a&gt;.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-11-05-oneapi_0.1/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-11-05-oneapi_0.1/</guid>
    <pubDate>Thu, 5 Nov 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[CUDA.jl 2.1]]></title>
    <link>https://juliagpu.org/post/2020-10-30-cuda_2.1/</link>
    <description><![CDATA[&lt;p&gt;CUDA.jl v2.1 is a bug-fix release, with one new feature: support for cubic texture interpolations. The release also partly reverts a change from v2.0: &lt;code&gt;reshape&lt;/code&gt;, &lt;code&gt;reinterpret&lt;/code&gt; and contiguous &lt;code&gt;view&lt;/code&gt;s now return a &lt;code&gt;CuArray&lt;/code&gt; again.&lt;/p&gt; &lt;h2 id&#61;&quot;generalized<em>texture</em>interpolations<strong>2&quot;&gt;&lt;a href&#61;&quot;#generalized<em>texture</em>interpolations</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Generalized texture interpolations&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDA&#39;s texture hardware only supports nearest-neighbour and linear interpolation, for other modes one is required to perform the interpolation by hand. In CUDA.jl v2.1 we are generalizing the texture interpolation API so that it is possible to use both hardware-backed and software-implemented interpolation modes in exactly the same way:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;# N is the dimensionality &#40;1, 2 or 3&#41;
<h1 id="t_is_the_element_type_needs_to_be_supported_by_the_texture_hardware"><a href="#t_is_the_element_type_needs_to_be_supported_by_the_texture_hardware" class="header-anchor">T is the element type &#40;needs to be supported by the texture hardware&#41;</a></h1>
<h1 id="source_array"><a href="#source_array" class="header-anchor">source array</a></h1>
src &#61; rand&#40;T, fill&#40;10, N&#41;...&#41;
<h1 id="indices_we_want_to_interpolate"><a href="#indices_we_want_to_interpolate" class="header-anchor">indices we want to interpolate</a></h1>
idx &#61; &#91;tuple&#40;rand&#40;1:0.1:10, N&#41;...&#41; for _ in 1:10&#93;
<h1 id="upload_to_the_gpu"><a href="#upload_to_the_gpu" class="header-anchor">upload to the GPU</a></h1>
gpu<em>src &#61; CuArray&#40;src&#41; gpu</em>idx &#61; CuArray&#40;idx&#41;
<h1 id="create_a_texture_array_for_optimized_fetching"><a href="#create_a_texture_array_for_optimized_fetching" class="header-anchor">create a texture array for optimized fetching</a></h1>
<h1 id="this_is_required_for_n1_optional_for_n2_and_n3"><a href="#this_is_required_for_n1_optional_for_n2_and_n3" class="header-anchor">this is required for N&#61;1, optional for N&#61;2 and N&#61;3</a></h1>
gpu<em>src &#61; CuTextureArray&#40;gpu</em>src&#41;
<h1 id="interpolate_using_a_texture"><a href="#interpolate_using_a_texture" class="header-anchor">interpolate using a texture</a></h1>
gpu<em>dst &#61; CuArray&#123;T&#125;&#40;undef, size&#40;gpu</em>idx&#41;&#41; gpu<em>tex &#61; CuTexture&#40;gpu</em>src; interpolation&#61;CUDA.NearestNeighbour&#40;&#41;&#41; broadcast&#33;&#40;gpu<em>dst, gpu</em>idx, Ref&#40;gpu_tex&#41;&#41; do idx, tex     tex&#91;idx...&#93; end
<h1 id="back_to_the_cpu"><a href="#back_to_the_cpu" class="header-anchor">back to the CPU</a></h1>
dst &#61; Array&#40;gpu<em>dst&#41;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, we can change the &lt;code&gt;interpolation&lt;/code&gt; argument to &lt;code&gt;CuTexture&lt;/code&gt; to either &lt;code&gt;NearestNeighbour&lt;/code&gt; or &lt;code&gt;LinearInterpolation&lt;/code&gt;, both supported by the hardware, or &lt;code&gt;CubicInterpolation&lt;/code&gt; which is implemented in software &#40;building on the hardware-supported linear interpolation&#41;.&lt;/p&gt; &lt;h2 id&#61;&quot;partial</em>revert<em>of</em>array<em>wrapper</em>changes<strong>2&quot;&gt;&lt;a href&#61;&quot;#partial<em>revert</em>of<em>array</em>wrapper_changes</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Partial revert of array wrapper changes&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;In CUDA.jl v2.0, we changed the behavior of several important array operations to reuse available wrappers in Base: &lt;code&gt;reshape&lt;/code&gt; started returning a &lt;code&gt;ReshapedArray&lt;/code&gt;, &lt;code&gt;view&lt;/code&gt; now returned a &lt;code&gt;SubArray&lt;/code&gt;, and &lt;code&gt;reinterpret&lt;/code&gt; was reworked to use &lt;code&gt;ReinterpretArray&lt;/code&gt;. These changes were made to ensure maximal compatibility with Base&#39;s array type, and to simplify the implementation in CUDA.jl and GPUArrays.jl.&lt;/p&gt; &lt;p&gt;However, this change turned out to regress the time to precompile and load CUDA.jl. Consequently, the change has been reverted, and these wrappers are now implemented as part of the &lt;code&gt;CuArray&lt;/code&gt; type again. Note however that we intend to revisit this change in the future. It is therefore recommended to use the &lt;code&gt;DenseCuArray&lt;/code&gt; type alias for methods that need a &lt;code&gt;CuArray&lt;/code&gt; backed by contiguous GPU memory. For strided &lt;code&gt;CuArray&lt;/code&gt;s, i.e. non-contiguous views, you should use the &lt;code&gt;StridedCuArray&lt;/code&gt; alias.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-10-30-cuda_2.1/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-10-30-cuda_2.1/</guid>
    <pubDate>Fri, 30 Oct 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[CUDA.jl 2.0]]></title>
    <link>https://juliagpu.org/post/2020-10-02-cuda_2.0/</link>
    <description><![CDATA[&lt;p&gt;Today we&#39;re releasing CUDA.jl 2.0, a breaking release with several new features. Highlights include initial support for Float16, a switch to CUDA&#39;s new stream model, a much-needed rework of the sparse array support and support for CUDA 11.1.&lt;/p&gt; &lt;p&gt;The release now requires &lt;strong&gt;Julia 1.5&lt;/strong&gt;, and assumes a GPU with &lt;strong&gt;compute capability 5.0&lt;/strong&gt; or higher &#40;although most of the package will still work with an older GPU&#41;.&lt;/p&gt; &lt;h2 id&#61;&quot;low-<em>and</em>mixed-precision<em>operations__2&quot;&gt;&lt;a href&#61;&quot;#low-</em>and<em>mixed-precision</em>operations__2&quot; class&#61;&quot;header-anchor&quot;&gt;Low- and mixed-precision operations&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;With NVIDIA&#39;s latest GPUs featuring more and more low-precision operations, CUDA.jl &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/417&quot;&gt;now&lt;/a&gt; starts to support these data types. For example, the CUBLAS wrappers can be used with &#40;B&#41;Float16 inputs &#40;running under &lt;code&gt;JULIA_DEBUG&#61;CUBLAS&lt;/code&gt; to illustrate the called methods&#41; thanks to the &lt;code&gt;cublasGemmEx&lt;/code&gt; API call:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; mul&#33;&#40;CUDA.zeros&#40;Float32,2,2&#41;,             cu&#40;rand&#40;Float16,2,2&#41;&#41;,             cu&#40;rand&#40;Float16,2,2&#41;&#41;&#41;
I&#33; cuBLAS &#40;v11.0&#41; function cublasStatus<em>t cublasGemmEx&#40;...&#41; called: i&#33;  Atype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>16F&#40;2&#41; i&#33;  Btype: type&#61;cudaDataType<em>t; val&#61;CUDA</em>R<em>16F&#40;2&#41; i&#33;  Ctype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>32F&#40;0&#41; i&#33;  computeType: type&#61;cublasComputeType<em>t; val&#61;CUBLAS</em>COMPUTE_32F&#40;68&#41;
2×2 CuArray&#123;Float32,2&#125;:  0.481284  0.561241  1.12923   1.04541&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; using BFloat16s
julia&gt; mul&#33;&#40;CUDA.zeros&#40;BFloat16,2,2&#41;,             cu&#40;BFloat16.&#40;rand&#40;2,2&#41;&#41;&#41;,             cu&#40;BFloat16.&#40;rand&#40;2,2&#41;&#41;&#41;&#41;
I&#33; cuBLAS &#40;v11.0&#41; function cublasStatus<em>t cublasGemmEx&#40;...&#41; called: i&#33;  Atype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>16BF&#40;14&#41; i&#33;  Btype: type&#61;cudaDataType<em>t; val&#61;CUDA</em>R<em>16BF&#40;14&#41; i&#33;  Ctype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>16BF&#40;14&#41; i&#33;  computeType: type&#61;cublasComputeType<em>t; val&#61;CUBLAS</em>COMPUTE_32F&#40;68&#41;
2×2 CuArray&#123;BFloat16,2&#125;:  0.300781   0.71875  0.0163574  0.0241699&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, CUBLAS can be configured to automatically down-cast 32-bit inputs to Float16. This is &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/424&quot;&gt;now&lt;/a&gt; exposed through a task-local CUDA.jl math mode:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; CUDA.math<em>mode&#33;&#40;CUDA.FAST</em>MATH; precision&#61;:Float16&#41;
julia&gt; mul&#33;&#40;CuArray&#40;zeros&#40;Float32,2,2&#41;&#41;,             CuArray&#40;rand&#40;Float32,2,2&#41;&#41;,             CuArray&#40;rand&#40;Float32,2,2&#41;&#41;&#41;
I&#33; cuBLAS &#40;v11.0&#41; function cublasStatus<em>t cublasGemmEx&#40;...&#41; called: i&#33;  Atype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>32F&#40;0&#41; i&#33;  Btype: type&#61;cudaDataType<em>t; val&#61;CUDA</em>R<em>32F&#40;0&#41; i&#33;  Ctype: type&#61;cudaDataType</em>t; val&#61;CUDA<em>R</em>32F&#40;0&#41; i&#33;  computeType: type&#61;cublasComputeType<em>t; val&#61;CUBLAS</em>COMPUTE<em>32F</em>FAST_16F&#40;74&#41;
2×2 CuArray&#123;Float32,2&#125;:  0.175258  0.226159  0.511893  0.331351&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As part of these changes, CUDA.jl now defaults to using tensor cores. This may affect accuracy; use math mode &lt;code&gt;PEDANTIC&lt;/code&gt; if you want the old behavior.&lt;/p&gt; &lt;p&gt;Work is &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/issues/391&quot;&gt;under way&lt;/a&gt; to extend these capabilities to the rest of CUDA.jl, e.g., the CUDNN wrappers, or the native kernel programming capabilities.&lt;/p&gt; &lt;h2 id&#61;&quot;new<em>default</em>stream<em>semantics__2&quot;&gt;&lt;a href&#61;&quot;#new</em>default<em>stream</em>semantics<strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;New default stream semantics&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;In CUDA.jl 2.0 we&#39;re &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/395&quot;&gt;switching&lt;/a&gt; to CUDA&#39;s &lt;a href&#61;&quot;https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/&quot;&gt;simplified stream programming model&lt;/a&gt;. This simplifies working with multiple streams, and opens up more possibilities for concurrent execution of GPU operations.&lt;/p&gt; &lt;h3 id&#61;&quot;multi-stream_programming</strong>2&quot;&gt;&lt;a href&#61;&quot;#multi-stream_programming__2&quot; class&#61;&quot;header-anchor&quot;&gt;Multi-stream programming&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;In the old model, the default stream &#40;used by all GPU operations unless specified otherwise&#41; was a special stream whose commands could not be executed concurrently with commands on regular, explicitly-created streams. For example, if we interleave kernels executed on a dedicated stream with ones on the default one, execution was serialized:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;using CUDA
N &#61; 1 &lt;&lt; 20
function kernel&#40;x, n&#41;     tid &#61; threadIdx&#40;&#41;.x &#43; &#40;blockIdx&#40;&#41;.x-1&#41;* blockDim&#40;&#41;.x     for i &#61; tid:blockDim&#40;&#41;.x*gridDim&#40;&#41;.x:n         x&#91;i&#93; &#61; CUDA.sqrt&#40;CUDA.pow&#40;3.14159f0, i&#41;&#41;     end     return end
num_streams &#61; 8
for i in 1:num_streams     stream &#61; CuStream&#40;&#41;
data &amp;#61; CuArray&amp;#123;Float32&amp;#125;&amp;#40;undef, N&amp;#41;

@cuda blocks&amp;#61;1 threads&amp;#61;64 stream&amp;#61;stream kernel&amp;#40;data, N&amp;#41;

@cuda kernel&amp;#40;data, 0&amp;#41;
end&lt;/code&gt;&lt;/pre&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-10-02-cuda<em>2.0/multistream</em>before.png&quot; alt&#61;&quot;Multi-stream programming &#40;old&#41;&quot;&gt; &lt;/figure&gt;
&lt;p&gt;In the new model, default streams are regular streams and commands issued on them can execute concurrently with those on other streams:&lt;/p&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-10-02-cuda<em>2.0/multistream</em>after.png&quot; alt&#61;&quot;Multi-stream programming &#40;new&#41;&quot;&gt; &lt;/figure&gt;
&lt;h3 id&#61;&quot;multi-threading<strong>2&quot;&gt;&lt;a href&#61;&quot;#multi-threading</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Multi-threading&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Another consequence of the new stream model is that each thread gets its own default stream &#40;accessible as &lt;code&gt;CuStreamPerThread&#40;&#41;&lt;/code&gt;&#41;. Together with Julia&#39;s threading capabilities, this makes it trivial to group independent work in tasks, benefiting from concurrent execution on the GPU where possible:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;using CUDA
N &#61; 1 &lt;&lt; 20
function kernel&#40;x, n&#41;     tid &#61; threadIdx&#40;&#41;.x &#43; &#40;blockIdx&#40;&#41;.x-1&#41;* blockDim&#40;&#41;.x     for i &#61; tid:blockDim&#40;&#41;.x*gridDim&#40;&#41;.x:n         x&#91;i&#93; &#61; CUDA.sqrt&#40;CUDA.pow&#40;3.14159f0, i&#41;&#41;     end     return end
Threads.@threads for i in 1:Threads.nthreads&#40;&#41;     data &#61; CuArray&#123;Float32&#125;&#40;undef, N&#41;     @cuda blocks&#61;1 threads&#61;64 kernel&#40;data, N&#41;     synchronize&#40;CuDefaultStream&#40;&#41;&#41; end&lt;/code&gt;&lt;/pre&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-10-02-cuda<em>2.0/multithread</em>after.png&quot; alt&#61;&quot;Multi-threading &#40;new&#41;&quot;&gt; &lt;/figure&gt;
&lt;p&gt;With the old model, execution would have been serialized because the default stream was the same across threads:&lt;/p&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-10-02-cuda<em>2.0/multithread</em>before.png&quot; alt&#61;&quot;Multi-threading &#40;old&#41;&quot;&gt; &lt;/figure&gt;
&lt;p&gt;Future improvements will make this behavior configurable, such that users can use a different default stream per task.&lt;/p&gt; &lt;h2 id&#61;&quot;sparse<em>array</em>clean-up<strong>2&quot;&gt;&lt;a href&#61;&quot;#sparse<em>array</em>clean-up</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Sparse array clean-up&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;As part of CUDA.jl 2.0, the sparse array support &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/409&quot;&gt;has been refactored&lt;/a&gt;, bringing them in line with other array types and their expected behavior. For example, the custom &lt;code&gt;switch2&lt;/code&gt; methods have been removed in favor of calls to &lt;code&gt;convert&lt;/code&gt; and array constructors:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; using SparseArrays julia&gt; using CUDA, CUDA.CUSPARSE
julia&gt; CuSparseMatrixCSC&#40;CUDA.rand&#40;2,2&#41;&#41; 2×2 CuSparseMatrixCSC&#123;Float32&#125; with 4 stored entries: &#91;1, 1&#93; &#61;  0.124012 &#91;2, 1&#93; &#61;  0.791714 &#91;1, 2&#93; &#61;  0.487905 &#91;2, 2&#93; &#61;  0.752466
julia&gt; CuSparseMatrixCOO&#40;sprand&#40;2,2, 0.5&#41;&#41; 2×2 CuSparseMatrixCOO&#123;Float64&#125; with 3 stored entries: &#91;1, 1&#93; &#61;  0.183183 &#91;2, 1&#93; &#61;  0.966466 &#91;2, 2&#93; &#61;  0.064101
julia&gt; CuSparseMatrixCSR&#40;ans&#41; 2×2 CuSparseMatrixCSR&#123;Float64&#125; with 3 stored entries: &#91;1, 1&#93; &#61;  0.183183 &#91;2, 1&#93; &#61;  0.966466 &#91;2, 2&#93; &#61;  0.064101&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/421&quot;&gt;Initial support for the COO sparse matrix type &lt;/a&gt; has also been added, along with more &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/351&quot;&gt;better support for sparse matrix-vector multiplication&lt;/a&gt;.&lt;/p&gt; &lt;h2 id&#61;&quot;support<em>for</em>cuda<em>111__2&quot;&gt;&lt;a href&#61;&quot;#support</em>for<em>cuda</em>111__2&quot; class&#61;&quot;header-anchor&quot;&gt;Support for CUDA 11.1&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This release also features support for the brand-new CUDA 11.1. As there is no compatible release of CUDNN or CUTENSOR yet, CUDA.jl won&#39;t automatically select this version, but you can force it to by setting the &lt;code&gt;JULIA<em>CUDA</em>VERSION&lt;/code&gt; environment variable to &lt;code&gt;11.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; ENV&#91;&quot;JULIA<em>CUDA</em>VERSION&quot;&#93; &#61; &quot;11.1&quot;
julia&gt; using CUDA
julia&gt; CUDA.versioninfo&#40;&#41; CUDA toolkit 11.1.0, artifact installation
Libraries:
<ul>
<li>CUDNN: missing
</li>
<li>CUTENSOR: missing&lt;/code&gt;&lt;/pre&gt;
</li>
</ul>
&lt;h2 id&#61;&quot;minor<em>changes__2&quot;&gt;&lt;a href&#61;&quot;#minor</em>changes__2&quot; class&#61;&quot;header-anchor&quot;&gt;Minor changes&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Many other changes are part of this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Views, reshapes and array reinterpretations &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/437&quot;&gt;are now represented&lt;/a&gt; by the Base array wrappers, simplifying the CuArray type definition.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;Various optimizations to &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/428&quot;&gt;CUFFT&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/321&quot;&gt;CUDNN&lt;/a&gt; library wrappers.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/427&quot;&gt;Support&lt;/a&gt; for &lt;code&gt;LinearAlgebra.reflect&#33;&lt;/code&gt; and &lt;code&gt;rotate&#33;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDA.jl/pull/435&quot;&gt;Initial support&lt;/a&gt; for calling CUDA libraries with strided inputs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;
<br><a href="https://juliagpu.org/post/2020-10-02-cuda_2.0/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-10-02-cuda_2.0/</guid>
    <pubDate>Fri, 2 Oct 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[Paper: Flexible Performant GEMM Kernels on GPUs]]></title>
    <link>https://juliagpu.org/post/2020-09-28-gemmkernels/</link>
    <description><![CDATA[&lt;p&gt;General Matrix Multiplication or GEMM kernels take center place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA&#39;s Tensor Cores. In this paper we show how it is possible to program these accelerators from Julia, and present abstractions and interfaces that allow to do so efficiently without sacrificing performance.&lt;/p&gt; &lt;p&gt;A pre-print of the paper has been published on arXiv: &lt;a href&#61;&quot;https://arxiv.org/abs/2009.12263&quot;&gt;arXiv:2009.12263&lt;/a&gt;. &lt;br&gt; The source code can be found on GitHub: &lt;a href&#61;&quot;https://github.com/thomasfaingnaert/GemmKernels.jl&quot;&gt;thomasfaingnaert/GemmKernels.jl&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;With the APIs from GemmKernels.jl, it is possible to instantiate GEMM kernels that perform in the same ball park as, and sometimes even outperform state-of-the-art libraries like CUBLAS and CUTLASS. For example, performing a mixed-precision multiplication of two 16-bit matrixes into a 32-bit accumulator &#40;on different combinations of layouts&#41;:&lt;/p&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-09-28-gemmkernels/mixed_precision.png&quot; alt&#61;&quot;Performance of mixed-precision GEMM&quot;&gt; &lt;/figure&gt;
&lt;p&gt;The APIs are also highly flexible and allow customization of each step, e.g., to apply the activation function &lt;code&gt;max&#40;x, 0&#41;&lt;/code&gt; for implementing a rectified linear unit &#40;ReLU&#41;:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;a &#61; CuArray&#40;rand&#40;Float16, &#40;M, K&#41;&#41;&#41; b &#61; CuArray&#40;rand&#40;Float16, &#40;K, N&#41;&#41;&#41; c &#61; CuArray&#40;rand&#40;Float32, &#40;M, N&#41;&#41;&#41; d &#61; similar&#40;c&#41;
conf &#61; GemmKernels.get<em>config&#40;     gemm</em>shape &#61; &#40;M &#61; M, N &#61; N, K &#61; K&#41;,     operator &#61; Operator.WMMAOp&#123;16, 16, 16&#125;,     global<em>a</em>layout &#61; Layout.AlignedColMajor&#123;Float16&#125;,     global<em>c</em>layout &#61; Layout.AlignedColMajor&#123;Float32&#125;&#41;
GemmKernels.matmul&#40;     a, b, c, d, conf;     transform<em>regs</em>to<em>shared</em>d &#61; Transform.Elementwise&#40;x -&gt; max&#40;x, 0&#41;&#41;&#41;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GemmKernels.jl framework is written entirely in Julia, demonstrating the high-performance GPU programming capabilities of this language, but at the same time keeping the research accessible and easy to modify or repurpose by other Julia developers.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-09-28-gemmkernels/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-09-28-gemmkernels/</guid>
    <pubDate>Mon, 28 Sep 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[CUDA.jl 1.3 - Multi-device programming]]></title>
    <link>https://juliagpu.org/post/2020-07-18-CUDA_1.3/</link>
    <description><![CDATA[&lt;p&gt;Today we&#39;re releasing CUDA.jl 1.3, with several new features. The most prominent change is support for multiple GPUs within a single process.&lt;/p&gt; &lt;h2 id&#61;&quot;multi-gpu<em>programming__2&quot;&gt;&lt;a href&#61;&quot;#multi-gpu</em>programming__2&quot; class&#61;&quot;header-anchor&quot;&gt;Multi-GPU programming&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;With CUDA.jl 1.3, you can finally use multiple CUDA GPUs within a single process. To switch devices you can call &lt;code&gt;device&#33;&lt;/code&gt;, query the current device with &lt;code&gt;device&#40;&#41;&lt;/code&gt;, or reset it using &lt;code&gt;device_reset&#33;&#40;&#41;&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; collect&#40;devices&#40;&#41;&#41; 9-element Array&#123;CuDevice,1&#125;:  CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB  CuDevice&#40;1&#41;: Tesla V100-PCIE-32GB  CuDevice&#40;2&#41;: Tesla V100-PCIE-32GB  CuDevice&#40;3&#41;: Tesla V100-PCIE-32GB  CuDevice&#40;4&#41;: Tesla V100-PCIE-16GB  CuDevice&#40;5&#41;: Tesla P100-PCIE-16GB  CuDevice&#40;6&#41;: Tesla P100-PCIE-16GB  CuDevice&#40;7&#41;: GeForce GTX 1080 Ti  CuDevice&#40;8&#41;: GeForce GTX 1080 Ti
julia&gt; device&#33;&#40;5&#41;
julia&gt; device&#40;&#41; CuDevice&#40;5&#41;: Tesla P100-PCIE-16GB&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let&#39;s define a kernel to show this really works:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; function kernel&#40;&#41;            dev &#61; Ref&#123;Cint&#125;&#40;&#41;            CUDA.cudaGetDevice&#40;dev&#41;            @cuprintln&#40;&quot;Running on device &#36;&#40;dev&#91;&#93;&#41;&quot;&#41;            return        end
julia&gt; @cuda kernel&#40;&#41; Running on device 5
julia&gt; device&#33;&#40;0&#41;
julia&gt; device&#40;&#41; CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB
julia&gt; @cuda kernel&#40;&#41; Running on device 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Memory allocations, like &lt;code&gt;CuArray&lt;/code&gt;s, are implicitly bound to the device they were allocated on. That means you should take care to only use an array when the owning device is active, or you will run into errors:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; device&#40;&#41; CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB
julia&gt; a &#61; CUDA.rand&#40;1&#41; 1-element CuArray&#123;Float32,1&#125;:  0.6322775
julia&gt; device&#33;&#40;1&#41;
julia&gt; a ERROR: CUDA error: an illegal memory access was encountered&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Future improvements might make the array type device-aware.&lt;/p&gt; &lt;h2 id&#61;&quot;multitasking<em>and</em>multithreading<strong>2&quot;&gt;&lt;a href&#61;&quot;#multitasking<em>and</em>multithreading</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Multitasking and multithreading&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Dovetailing with the support for multiple GPUs, is the ability to use these GPUs on separate Julia tasks and threads:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; device&#33;&#40;0&#41;
julia&gt; @sync begin          @async begin            device&#33;&#40;1&#41;            println&#40;&quot;Working with &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current<em>task&#40;&#41;&#41;&quot;&#41;            yield&#40;&#41;            println&#40;&quot;Back to device &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current</em>task&#40;&#41;&#41;&quot;&#41;          end          @async begin            device&#33;&#40;2&#41;            println&#40;&quot;Working with &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current_task&#40;&#41;&#41;&quot;&#41;          end        end Working with CuDevice&#40;1&#41; on Task @0x00007fc9e6a48010 Working with CuDevice&#40;2&#41; on Task @0x00007fc9e6a484f0 Back to device CuDevice&#40;1&#41; on Task @0x00007fc9e6a48010
julia&gt; device&#40;&#41; CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each task has its own local GPU state, such as the device it was bound to, handles to libraries like CUBLAS or CUDNN &#40;which means that each task can configure libraries independently&#41;, etc.&lt;/p&gt; &lt;h2 id&#61;&quot;minor<em>features__2&quot;&gt;&lt;a href&#61;&quot;#minor</em>features<strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Minor features&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDA.jl 1.3 also features some minor changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Reinstated compatibility with Julia 1.3&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;Support for CUDA 11.0 Update 1&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;Support for CUDNN 8.0.2&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id&#61;&quot;known_issues</strong>2&quot;&gt;&lt;a href&#61;&quot;#known_issues__2&quot; class&#61;&quot;header-anchor&quot;&gt;Known issues&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Several operations on sparse arrays have been broken since CUDA.jl 1.2, due to the deprecations that were part of CUDA 11. The next version of CUDA.jl will drop support for CUDA 10.0 or older, which will make it possible to use new cuSPARSE APIs and add back missing functionality.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-07-18-CUDA_1.3/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-07-18-CUDA_1.3/</guid>
    <pubDate>Sat, 18 Jul 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[CUDA.jl 1.1]]></title>
    <link>https://juliagpu.org/post/2020-07-07-cuda_1.1/</link>
    <description><![CDATA[&lt;p&gt;CUDA.jl 1.1 marks the first feature release after merging several CUDA packages into one. It raises the minimal Julia version to 1.4, and comes with support for the impending 1.5 release.&lt;/p&gt; &lt;h2 id&#61;&quot;cudajl<em>replacing</em>cuarrayscudanativejl<strong>2&quot;&gt;&lt;a href&#61;&quot;#cudajl<em>replacing</em>cuarrayscudanativejl</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;CUDA.jl replacing CuArrays/CUDAnative.jl&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;As &lt;a href&#61;&quot;https://discourse.julialang.org/t/psa-cuda-jl-replacing-cuarrays-jl-cudanative-jl-cudadrv-jl-cudaapi-jl-call-for-testing/40205&quot;&gt;announced a while back&lt;/a&gt;, CUDA.jl is now the new package for programming CUDA GPUs in Julia, replacing CuArrays.jl, CUDAnative.jl, CUDAdrv.jl and CUDAapi.jl. The merged package should be a drop-in replacement: All existing functionality has been ported, and almost all exported functions are still there. Applications like Flux.jl or the DiffEq.jl stack are being updated to support this change.&lt;/p&gt; &lt;h2 id&#61;&quot;cuda<em>11</em>support<strong>2&quot;&gt;&lt;a href&#61;&quot;#cuda<em>11</em>support</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;CUDA 11 support&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;With CUDA.jl 1.1, we support the upcoming release of the CUDA toolkit. This only applies to locally-installed versions of the toolkit, i.e., you need to specify &lt;code&gt;JULIA<em>CUDA</em>USE<em>BINARYBUILDER&#61;false&lt;/code&gt; in your environment to pick up the locally-installed release candidate of the CUDA toolkit. New features, like the third-generation tensor cores and its extended type support, or any new APIs, are not yet natively supported by Julia code.&lt;/p&gt; &lt;h2 id&#61;&quot;nvidia</em>management<em>library</em>nvml<strong>2&quot;&gt;&lt;a href&#61;&quot;#nvidia<em>management</em>library_nvml</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;NVIDIA Management Library &#40;NVML&#41;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDA.jl now integrates with the NVIDIA Management Library, or NVML. With this library, it&#39;s possible to query information about the system, any GPU devices, their topology, etc.:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; using CUDA
julia&gt; dev &#61; first&#40;NVML.devices&#40;&#41;&#41; CUDA.NVML.Device&#40;Ptr&#123;Nothing&#125; @0x00007f987c7c6e38&#41;
julia&gt; NVML.uuid&#40;dev&#41; UUID&#40;&quot;b8d5e790-ea4d-f962-e0c3-0448f69f2e23&quot;&#41;
julia&gt; NVML.name&#40;dev&#41; &quot;Quadro RTX 5000&quot;
julia&gt; NVML.power_usage&#40;dev&#41; 37.863
julia&gt; NVML.energy<em>consumption&#40;dev&#41; 65330.292&lt;/code&gt;&lt;/pre&gt; &lt;h2 id&#61;&quot;experimental</em>texture<em>support__2&quot;&gt;&lt;a href&#61;&quot;#experimental</em>texture_support__2&quot; class&#61;&quot;header-anchor&quot;&gt;Experimental: Texture support&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;It is now also possible to use the GPU&#39;s hardware texture support from Julia, albeit using a fairly low-level and still experimental API &#40;many thanks to &lt;a href&#61;&quot;https://github.com/cdsousa&quot;&gt;@cdsousa&lt;/a&gt; for the initial development&#41;. As a demo, let&#39;s start with loading a sample image:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;julia&gt; using Images, TestImages, ColorTypes, FixedPointNumbers julia&gt; img &#61; RGBA&#123;N0f8&#125;.&#40;testimage&#40;&quot;lighthouse&quot;&#41;&#41;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We use RGBA since CUDA&#39;s texture hardware only supports 1, 2 or 4 channels. This support is also currently limited to &quot;plain&quot; types, so let&#39;s reinterpret the image:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;julia&gt; img′ &#61; reinterpret&#40;NTuple&#123;4,UInt8&#125;, img&#41;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we can upload this image to the array, using the &lt;code&gt;CuTextureArray&lt;/code&gt; type for optimized storage &#40;normal &lt;code&gt;CuArray&lt;/code&gt;s are supported too&#41;, and bind it to a &lt;code&gt;CuTexture&lt;/code&gt; object that we can pass to a kernel:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; texturearray &#61; CuTextureArray&#40;img′&#41;
julia&gt; texture &#61; CuTexture&#40;texturearray; normalized<em>coordinates&#61;true&#41; 512×768 4-channel CuTexture&#40;::CuTextureArray&#41; with eltype NTuple&#123;4,UInt8&#125;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let&#39;s write and a kernel that warps this image. Since we specified &lt;code&gt;normalized</em>coordinates&#61;true&lt;/code&gt;, we index the texture using values in &lt;code&gt;&#91;0,1&#93;&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;function warp&#40;dst, texture&#41;     tid &#61; threadIdx&#40;&#41;.x &#43; &#40;blockIdx&#40;&#41;.x - 1&#41;* blockDim&#40;&#41;.x     I &#61; CartesianIndices&#40;dst&#41;     @inbounds if tid &lt;&#61; length&#40;I&#41;         i,j &#61; Tuple&#40;I&#91;tid&#93;&#41;         u &#61; Float32&#40;i-1&#41; / Float32&#40;size&#40;dst, 1&#41;-1&#41;         v &#61; Float32&#40;j-1&#41; / Float32&#40;size&#40;dst, 2&#41;-1&#41;         x &#61; u &#43; 0.02f0 * CUDA.sin&#40;30v&#41;         y &#61; v &#43; 0.03f0 * CUDA.sin&#40;20u&#41;         dst&#91;i,j&#93; &#61; texture&#91;x,y&#93;     end     return end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The size of the output image determines how many elements we need to process. This needs to be translated to a number of threads and blocks, keeping in mind device and kernel characteristics. We automate this using the occupancy API:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; outimg_d &#61; CuArray&#123;eltype&#40;img′&#41;&#125;&#40;undef, 500, 1000&#41;;
julia&gt; function configurator&#40;kernel&#41;            config &#61; launch_configuration&#40;kernel.fun&#41;
       threads &amp;#61; Base.min&amp;#40;length&amp;#40;outimg_d&amp;#41;, config.threads&amp;#41;
       blocks &amp;#61; cld&amp;#40;length&amp;#40;outimg_d&amp;#41;, threads&amp;#41;

       return &amp;#40;threads&amp;#61;threads, blocks&amp;#61;blocks&amp;#41;
   end
julia&gt; @cuda config&#61;configurator warp&#40;outimg<em>d, texture&#41;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, we fetch and visualize the output:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; outimg &#61; Array&#40;outimg</em>d&#41;
julia&gt; save&#40;&quot;imgwarp.png&quot;, reinterpret&#40;eltype&#40;img&#41;, outimg&#41;&#41;&lt;/code&gt;&lt;/pre&gt; &lt;figure&gt;   &lt;img src&#61;&quot;/post/2020-07-07-cuda_1.1/imgwarp.png&quot; alt&#61;&quot;Warped lighthouse&quot;&gt; &lt;/figure&gt;
&lt;h2 id&#61;&quot;minor<em>features__2&quot;&gt;&lt;a href&#61;&quot;#minor</em>features__2&quot; class&#61;&quot;header-anchor&quot;&gt;Minor features&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;The test-suite is now parallelized, using up-to &lt;code&gt;JULIA<em>NUM</em>THREADS&lt;/code&gt; processes:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;&#36; JULIA<em>NUM</em>THREADS&#61;4 julia -e &#39;using Pkg; Pkg.test&#40;&quot;CUDA&quot;&#41;;&#39;
                                 |          | ---------------- GPU ---------------- | ---------------- CPU ---------------- |
Test &#40;Worker&#41; | Time &#40;s&#41; | GC &#40;s&#41; | GC &#37; | Alloc &#40;MB&#41; | RSS &#40;MB&#41; | GC &#40;s&#41; | GC &#37; | Alloc &#40;MB&#41; | RSS &#40;MB&#41; | initialization &#40;2&#41; |     2.52 |   0.00 |  0.0 |       0.00 |   115.00 |   0.05 |  1.8 |     153.13 |   546.27 | apiutils &#40;4&#41; |     0.55 |   0.00 |  0.0 |       0.00 |   115.00 |   0.02 |  4.0 |      75.86 |   522.36 | codegen &#40;4&#41; |    14.81 |   0.36 |  2.5 |       0.00 |   157.00 |   0.62 |  4.2 |    1592.28 |   675.15 | ... gpuarrays/mapreduce essentials &#40;2&#41; |   113.52 |   0.01 |  0.0 |       3.19 |   641.00 |   2.61 |  2.3 |    8232.84 |  2449.35 | gpuarrays/mapreduce &#40;old tests&#41; &#40;5&#41; |   138.35 |   0.01 |  0.0 |     130.20 |   507.00 |   2.94 |  2.1 |    8615.15 |  2353.62 | gpuarrays/mapreduce derivatives &#40;3&#41; |   180.52 |   0.01 |  0.0 |       3.06 |   229.00 |   3.44 |  1.9 |   12262.67 |  1403.39 |
Test Summary: |  Pass  Broken  Total   Overall     | 11213       3  11216     SUCCESS     Testing CUDA tests passed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A copy of &lt;code&gt;Base.versioninfo&#40;&#41;&lt;/code&gt; is available to report on the CUDA toolchain and any devices:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; CUDA.versioninfo&#40;&#41; CUDA toolkit 10.2.89, artifact installation CUDA driver 11.0.0 NVIDIA driver 450.36.6
Libraries:
<ul>
<li>CUBLAS: 10.2.2
</li>
<li>CURAND: 10.1.2
</li>
<li>CUFFT: 10.1.2
</li>
<li>CUSOLVER: 10.3.0
</li>
<li>CUSPARSE: 10.3.1
</li>
<li>CUPTI: 12.0.0
</li>
<li>NVML: 11.0.0&#43;450.36.6
</li>
<li>CUDNN: 7.6.5 &#40;for CUDA 10.2.0&#41;
</li>
<li>CUTENSOR: 1.1.0 &#40;for CUDA 10.2.0&#41;
</li>
</ul>
Toolchain:
<ul>
<li>Julia: 1.5.0-rc1.0
</li>
<li>LLVM: 9.0.1
</li>
<li>PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4
</li>
<li>Device support: sm<em>35, sm</em>37, sm<em>50, sm</em>52, sm<em>53, sm</em>60, sm<em>61, sm</em>62, sm<em>70, sm</em>72, sm_75
</li>
</ul>
1 device&#40;s&#41;:
<ul>
<li>Quadro RTX 5000 &#40;sm_75, 14.479 GiB / 15.744 GiB available&#41;&lt;/code&gt;&lt;/pre&gt;
</li>
</ul>
&lt;p&gt;CUTENSOR artifacts have been upgraded to version 1.1.0.&lt;/p&gt; &lt;p&gt;Benchmarking infrastructure based on the Codespeed project has been set-up at &lt;a href&#61;&quot;https://speed.juliagpu.org/&quot;&gt;speed.juliagpu.org&lt;/a&gt; to keep track of the performance of various operations.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-07-07-cuda_1.1/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-07-07-cuda_1.1/</guid>
    <pubDate>Tue, 7 Jul 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[CUDAnative.jl 3.0 and CuArrays.jl 2.0]]></title>
    <link>https://juliagpu.org/post/2020-03-25-cudanative_3.0-cuarrays_2.0/</link>
    <description><![CDATA[&lt;p&gt;This release of the Julia CUDA stack contains some exciting new features: automatic installation of CUDA using artifacts, full support for GPU method redefinitions, and experimental support for multitasking and multithreading. The release is technically breaking, but most end-users should not be affected.&lt;/p&gt; &lt;h2 id&#61;&quot;api<em>changes__2&quot;&gt;&lt;a href&#61;&quot;#api</em>changes<strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;API changes&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Changes to certain APIs require these releases to be breaking, however, most users should not be affected and chances are you can just bump your Compat entries without any additional changes. Flux.jl users will have to wait a little longer though, as the package uses non-public APIs that have changed and &lt;a href&#61;&quot;https://github.com/FluxML/Flux.jl/pull/1050&quot;&gt;requires an update&lt;/a&gt;.&lt;/p&gt; &lt;h2 id&#61;&quot;artifacts</strong>2&quot;&gt;&lt;a href&#61;&quot;#artifacts__2&quot; class&#61;&quot;header-anchor&quot;&gt;Artifacts&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDA and its dependencies will now be automatically installed using artifacts generated by BinaryBuilder.jl. This greatly improves usability, and only requires a functioning NVIDIA driver:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; ENV&#91;&quot;JULIA_DEBUG&quot;&#93; &#61; &quot;CUDAnative&quot;
julia&gt; using CUDAnative
julia&gt; CUDAnative.version&#40;&#41; ┌ Debug: Trying to use artifacts... └ @ CUDAnative CUDAnative/src/bindeps.jl:52 ┌ Debug: Using CUDA 10.2.89 from an artifact at /depot/artifacts/... └ @ CUDAnative CUDAnative/src/bindeps.jl:108 v&quot;10.2.89&quot;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use of a local installation is still possible by setting the environment variable &lt;code&gt;JULIA<em>CUDA</em>USE<em>BINARYBUILDER&lt;/code&gt; to false. For more details, refer to &lt;a href&#61;&quot;https://juliagpu.gitlab.io/CUDA.jl/installation/overview/&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Relevant PRs: &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDAnative.jl/pull/492&quot;&gt;CUDAnative.jl#492&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/pull/490&quot;&gt;CuArrays.jl#490&lt;/a&gt;&lt;/p&gt; &lt;h2 id&#61;&quot;method</em>redefinitions<strong>2&quot;&gt;&lt;a href&#61;&quot;#method_redefinitions</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Method redefinitions&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;CUDAnative 3.0 now fully supports method redefinitions, commonly referred to as &lt;a href&#61;&quot;https://github.com/JuliaLang/julia/issues/265&quot;&gt;Julia issue #265&lt;/a&gt;, and makes it possible to use interactive programming tools like Revise.jl:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia-repl&quot;&gt;julia&gt; child&#40;&#41; &#61; 0 julia&gt; parent&#40;&#41; &#61; &#40;@cuprintln&#40;child&#40;&#41;&#41;; return&#41; julia&gt; @cuda parent&#40;&#41; 0
julia&gt; parent&#40;&#41; &#61; &#40;@cuprintln&#40;child&#40;&#41; &#43; 1&#41;; return&#41; julia&gt; @cuda parent&#40;&#41; 1
julia&gt; child&#40;&#41; &#61; 1 julia&gt; @cuda parent&#40;&#41; 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Relevant PRs: &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDAnative.jl/pull/581&quot;&gt;CUDAnative.jl#581&lt;/a&gt;&lt;/p&gt; &lt;h2 id&#61;&quot;experimental<em>multitasking</em>and<em>multithreading__2&quot;&gt;&lt;a href&#61;&quot;#experimental</em>multitasking<em>and</em>multithreading<strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Experimental: Multitasking and multithreading&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;With CUDAnative 3.0 and CuArrays 2.0 you can now use Julia tasks and threads to organize your code. In combination with CUDA streams, this makes it possible to execute kernels and other GPU operations in parallel:&lt;/p&gt; &lt;pre&gt;&lt;code class&#61;&quot;language-julia&quot;&gt;@sync begin     function my<em>expensive</em>kernel&#40;&#41;         return     end     @async @cuda stream&#61;CuStream&#40;&#41; my<em>expensive</em>kernel&#40;&#41;     @async @cuda stream&#61;CuStream&#40;&#41; my<em>expensive</em>kernel&#40;&#41; end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Every task, whether it runs on a separate thread or not, can work with a different device, as well as independently work with CUDA libraries like CUBLAS and CUFFT.&lt;/p&gt; &lt;p&gt;Note that this support is experimental, and lacks certain features to be fully effective. For one, the CuArrays memory allocator is not device-aware, and it is currently not possible to configure the CUDA stream for operations like map or broadcast.&lt;/p&gt; &lt;p&gt;Relevant PRs: &lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDAnative.jl/pull/609&quot;&gt;CUDAnative.jl#609&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/pull/645&quot;&gt;CuArrays.jl#645&lt;/a&gt;&lt;/p&gt; &lt;h2 id&#61;&quot;minor_changes</strong>2&quot;&gt;&lt;a href&#61;&quot;#minor<em>changes__2&quot; class&#61;&quot;header-anchor&quot;&gt;Minor changes&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;GPU kernels are now name-mangled like C&#43;&#43;, which offers better integration with NVIDIA tools &#40;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDAnative.jl/pull/559&quot;&gt;CUDAnative.jl#559&lt;/a&gt;&#41;.&lt;/p&gt; &lt;p&gt;A better N-dimensional &lt;code&gt;mapreducedim&#33;&lt;/code&gt; kernel, properly integrating with all Base interfaces &#40;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/pull/602&quot;&gt;CuArrays.jl#602&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/JuliaGPU/GPUArrays.jl/pull/246&quot;&gt;GPUArrays#246&lt;/a&gt;&#41;.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;CuIterator&lt;/code&gt; type for batching arrays to the GPU &#40;by @jrevels, &lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/pull/467&quot;&gt;CuArrays.jl#467&lt;/a&gt;&#41;.&lt;/p&gt; &lt;p&gt;Integration with Base&#39;s 5-arg &lt;code&gt;mul&#33;&lt;/code&gt; &#40;by @haampie, &lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/pull/641&quot;&gt;CuArrays.jl#641&lt;/a&gt; and &lt;a href&#61;&quot;https://github.com/JuliaGPU/GPUArrays.jl/pull/253&quot;&gt;GPUArrays#253&lt;/a&gt;&#41;.&lt;/p&gt; &lt;p&gt;Integration with Cthulhu.jl for interactive inspection of generated code &#40;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CUDAnative.jl/issues/597&quot;&gt;CUDAnative.jl#597&lt;/a&gt;&#41;.&lt;/p&gt; &lt;h2 id&#61;&quot;known</em>issues<strong>2&quot;&gt;&lt;a href&#61;&quot;#known_issues</strong>2&quot; class&#61;&quot;header-anchor&quot;&gt;Known issues&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;With a release as big as this one there&#39;s bound to be some bugs, e.g., with the installation of artifacts on exotic systems, or due to the many changes to make the libraries thread-safe. If you need absolute stability, please wait for a point release.&lt;/p&gt; &lt;p&gt;There are also some known issues. CUDAnative is currently not compatible with Julia 1.5 due to Base compiler changes &#40;&lt;a href&#61;&quot;https://github.com/JuliaLang/julia/issues/34993&quot;&gt;julia#34993&lt;/a&gt;&#41;, the new &lt;code&gt;mapreducedim&#33;&lt;/code&gt; kernel appears to be slower in some cases &#40;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/issues/611&quot;&gt;CuArrays.jl#611&lt;/a&gt;&#41;, and there are some remaining thread-safety issues when using the non-default memory pool &#40;&lt;a href&#61;&quot;https://github.com/JuliaGPU/CuArrays.jl/issues/647&quot;&gt;CuArrays.jl#647&lt;/a&gt;&#41;.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2020-03-25-cudanative_3.0-cuarrays_2.0/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2020-03-25-cudanative_3.0-cuarrays_2.0/</guid>
    <pubDate>Wed, 25 Mar 2020 00:00:00 UT</pubDate>
  </item>
  <item>
    <title><![CDATA[New website for JuliaGPU]]></title>
    <link>https://juliagpu.org/post/2019-12-12-new_site/</link>
    <description><![CDATA[&lt;p&gt;Welcome to the new landing page for the JuliaGPU organization. This website serves as an introduction to the several packages for programming GPUs in Julia, with pointers to relevant resources for new users.&lt;/p&gt; &lt;p&gt;The sources for this website are hosted at &lt;a href&#61;&quot;https://github.com/JuliaGPU/juliagpu.org&quot;&gt;GitHub&lt;/a&gt; and generated using Hugo, feel free to open an issue or pull request if you think it could be improved.&lt;/p&gt;
<br><a href="https://juliagpu.org/post/2019-12-12-new_site/">Read more</a>]]></description>
    <guid>https://juliagpu.org/post/2019-12-12-new_site/</guid>
    <pubDate>Thu, 12 Dec 2019 00:00:00 UT</pubDate>
  </item>
</channel>
</rss>
