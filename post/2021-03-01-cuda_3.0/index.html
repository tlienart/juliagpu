<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/juliagpu/assets/favicon.ico">
  <link rel=alternate type=application/rss+xml href=https://juliagpu.org/index.xml title=JuliaGPU>

  <link rel=stylesheet href="/juliagpu/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/juliagpu/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1 a, a:hover, a:visited, a:active {
   color: inherit;
   text-decoration: none;
}
#main h2 a, a:hover, a:visited, a:active {
   color: inherit;
   text-decoration: none;
}
#main h3 a, a:hover, a:visited, a:active {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>CUDA.jl 3.0 â‹… JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item active"><a class=nav-link href="/juliagpu/><i data-feather=home></i>Home</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/post/><i data-feather=file-text></i>Blog</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/learn/><i data-feather=book-open></i>Learn</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/cuda/>NVIDIA CUDA</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/rocm/>AMD ROCm</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/oneapi/>Intel oneAPI</a>
            </li>
            <li class=nav-item><a class=nav-link href="/juliagpu/other/>Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
    <h1>CUDA.jl 3.0</h1>
    <i data-feather=calendar></i>
<time datetime=2021-3-1>Mar 1, 2021</time>

    <br><br>
    
<!-- Content appended here -->

<p>CUDA.jl 3.0 is a significant, semi-breaking release that features greatly improved multi-tasking and multi-threading, support for CUDA 11.2 and its new memory allocator, and a completely revamped cuDNN interface.</p>
<h2 id="improved_multi-tasking_and_multi-threading"><a href="#improved_multi-tasking_and_multi-threading" class="header-anchor">Improved multi-tasking and multi-threading</a></h2>
<p>Traditionally, CUDA operations were enqueued on the global default stream, and many of these operations &#40;like copying memory, or synchronizing execution&#41; were fully blocking. This posed difficulties when using multiple tasks, each of which assumed to perform independent operations &#40;possibly on different devices, maybe even from a different CPU thread&#41;. <strong>CUDA.jl now uses a private stream for each Julia task, and avoids blocking operations where possible, enabling task-based concurrent execution.</strong></p>
<p>A ~~picture~~ snippet of code is worth a thousand words, so let&#39;s have a look at some dummy computation that both uses a library function &#40;GEMM from CUBLAS&#41; and a native Julia broadcast kernel:</p>
<pre><code class="language-julia">using CUDA, LinearAlgebra

function compute&#40;a,b,c&#41;
    mul&#33;&#40;c, a, b&#41;
    broadcast&#33;&#40;sin, c, c&#41;
    synchronize&#40;&#41;
    c
end</code></pre>
<p>To perform this computation concurrently, we can just use Julia&#39;s task-based programming interfaces and wrap each computation in an <code>@async</code> block. To synchronize these tasks at the end, we wrap in a <code>@sync</code> block. Finally, to visualize these computations in the profiler trace below we use NVTX&#39;s <code>@range</code> macro:</p>
<pre><code class="language-julia">function iteration&#40;a,b,c&#41;
    results &#61; Vector&#123;Any&#125;&#40;undef, 2&#41;
    NVTX.@range &quot;computation&quot; @sync begin
        @async begin
            results&#91;1&#93; &#61; compute&#40;a,b,c&#41;
        end
        @async begin
            results&#91;2&#93; &#61; compute&#40;a,b,c&#41;
        end
    end
    NVTX.@range &quot;comparison&quot; Array&#40;results&#91;1&#93;&#41; &#61;&#61; Array&#40;results&#91;2&#93;&#41;
end</code></pre>
<p>We then invoke this function using some random data:</p>
<pre><code class="language-julia">function main&#40;N&#61;1024&#41;
    a &#61; CUDA.rand&#40;N,N&#41;
    b &#61; CUDA.rand&#40;N,N&#41;
    c &#61; CUDA.rand&#40;N,N&#41;

    # make sure this data can be used by other tasks&#33;
    synchronize&#40;&#41;

    # warm-up
    iteration&#40;a,b,c&#41;
    GC.gc&#40;true&#41;

    NVTX.@range &quot;main&quot; iteration&#40;a,b,c&#41;
end</code></pre>
<p>The snippet above illustrates one breaking aspect of this change: Because each task uses its own stream, <strong>you now need to synchronize when re-using data in another task.</strong> Although it is unlikely that any user code was relying on the old behavior, it is technically a breaking change.</p>
<p>If we profile these computations, we can see how the execution was overlapped:</p>
<p>The region highlighted in green was spent enqueueing operations, which includes the call to <code>synchronize&#40;&#41;</code>. This used to be a globally-synchronizing operation, whereas now it only synchronizes the task-local stream while yielding to the Julia scheduler so that it can continue executing other tasks. <strong>For synchronizing the entire device, use the new <code>synchronize_all&#40;&#41;</code> function.</strong></p>
<p>The remainder of computation was then spent executing kernels. Here, these executions were overlapping, but that obviously depends on the exact characteristics of the computations and your GPU. The same approach however can be used for multi-GPU programming, where each task targets a different device. It is also possible to use different threads for each task.</p>
<p>Note that not all operations have been made fully asynchronous yet. For example, copying memory to or from the GPU will currently still synchronize execution, and thus inhibit switching to another task. That is why the example above only called <code>synchronize&#40;&#41;</code> in the compute task, and copied the memory to the host in the parent task.</p>
<h2 id="cuda_112_and_stream-ordered_allocations"><a href="#cuda_112_and_stream-ordered_allocations" class="header-anchor">CUDA 11.2 and stream-ordered allocations</a></h2>
<p>CUDA.jl now also fully supports CUDA 11.2, and it will default to using that version of the toolkit if your driver supports it. The release came with several new features, such as <a href="https://developer.nvidia.com/blog/enhancing-memory-allocation-with-new-cuda-11-2-features/">the new stream-ordered memory allocator</a>. Without going into details, it is now possible to asynchonously allocate memory, obviating much of the need to cache those allocations in a memory pool. Initial benchmarks have shown nice speed-ups from using this allocator, while lowering memory pressure and thus reducing invocations of the Julia garbage collector.</p>
<p>When using CUDA 11.2, CUDA.jl will default to the CUDA-backed memory pool, and disable its own caching layer. If you want to compare performance, you can still use the old allocator and caching memory pool by setting the <code>JULIA_CUDA_MEMORY_POOL</code> environment variable to, e.g. <code>binned</code>. On older versions of CUDA, the <code>binned</code> pool is still used by default.</p>
<h2 id="revamped_cudnn_interface"><a href="#revamped_cudnn_interface" class="header-anchor">Revamped cuDNN interface</a></h2>
<p>As part of this release, the cuDNN wrappers have been completely revamped by <a href="https://github.com/denizyuret">@denizyuret</a>. The goal of the redesign is to more faithfully map the cuDNN API to more natural Julia functions, so that packages like Knet.jl or NNlib.jl can more easily use advanced cuDNN features without having to resort to low-level C calls. For more details, refer to <a href="https://github.com/JuliaGPU/CUDA.jl/blob/da7c6eee82d6ea0eee1cb75c8589c8a92b0bc474/lib/cudnn/README.md">the design document</a>.</p>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/juliagpu/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <!-- <script src="/juliagpu/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script> -->

    <!-- GOOGLE ANALYTICS -->
    <!-- <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script> -->

  </body>
</html>
